# Image-to-Text-Generation-A-Vision-Language-Model-for-Automated-Image-Captioning

This project highlights the effectiveness of automated image captioning using Bootstrapping Language-Image Pretraining(BLIP), a state-of-the-art vision language model. The primary objective is to generate accurate and meaningful natural language descriptions of images by fine-tuning the pretrained BLIP model on widely used Flickr30k dataset. I have generated a user interface where users are allowed to upload an image, and instantly receive captions generated by the fine-tuned BLIP model. To evaluate the model performance, the model was evaluated using standard natural language generation metrics, achieving an average BLEU score of 0.0498 and a ROUGE score of 0.2566. These results indicate a decent level of precision and semantic similarity between the generated and reference captions. Overall, the project demonstrates both the feasibility and effectiveness of integrating advanced multimodal models into interactive tools that can enhance human-AI collaboration in image understanding tasks.
